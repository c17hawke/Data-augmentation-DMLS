{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT directory exists hence changing directory to 'Data-augmentation-DMLS'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT = Path(\"/Users/Admin/Desktop/PARA-OS/01_Projects/Learning-Design-ML-Systems/Data-augmentation-DMLS\")\n",
    "\n",
    "if ROOT.is_dir():\n",
    "    print(f\"ROOT directory exists hence changing directory to '{ROOT.name}'\")\n",
    "    os.chdir(ROOT)\n",
    "else:\n",
    "    raise NotADirectoryError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References - \n",
    "- [1] NLPAUG - https://pypi.org/project/nlpaug/\n",
    "- [2] NLPAUG Cookbook - https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jump over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "import nltk # noqa\n",
    "from nltk.corpus import wordnet # noqa\n",
    "\n",
    "def replace_synonyms(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms and synonyms[0].lemmas():\n",
    "            new_words.append(synonyms[0].lemmas()[0].name())\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "new_sentence = replace_synonyms(sentence)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('jump.n.01'),\n",
       " Synset('leap.n.02'),\n",
       " Synset('jump.n.03'),\n",
       " Synset('startle.n.01'),\n",
       " Synset('jump.n.05'),\n",
       " Synset('jump.n.06'),\n",
       " Synset('jump.v.01'),\n",
       " Synset('startle.v.02'),\n",
       " Synset('jump.v.03'),\n",
       " Synset('jump.v.04'),\n",
       " Synset('leap_out.v.01'),\n",
       " Synset('jump.v.06'),\n",
       " Synset('rise.v.11'),\n",
       " Synset('jump.v.08'),\n",
       " Synset('derail.v.02'),\n",
       " Synset('chute.v.01'),\n",
       " Synset('jump.v.11'),\n",
       " Synset('jumpstart.v.01'),\n",
       " Synset('jump.v.13'),\n",
       " Synset('leap.v.02'),\n",
       " Synset('alternate.v.01')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"jump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"jump\")[0].lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog .\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw # noqa\n",
    "\n",
    "text = 'The quick brown fox jumps over the lazy dog .'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog .\n",
      "Augmented Text:\n",
      "['The quick brownness fox jump terminated the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog .\n",
      "\n",
      "Augmented Text Sample 01:-\n",
      "The ready brown dodger jumps ended the lazy dog.\n",
      "----\n",
      "\n",
      "Augmented Text Sample 02:-\n",
      "The speedy brown charles james fox jumps terminated the lazy dog.\n",
      "----\n",
      "\n",
      "Augmented Text Sample 03:-\n",
      "The agile brown fox jumps ended the lazy frank.\n",
      "----\n",
      "\n",
      "Augmented Text Sample 04:-\n",
      "The quick brown dodger leap over the lazy hot dog.\n",
      "----\n",
      "\n",
      "Augmented Text Sample 05:-\n",
      "The straightaway brown fox skip over the work shy dog.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "for idx in range(1,6):\n",
    "    print()\n",
    "    augmented_text = aug.augment(text)\n",
    "    print(f'Augmented Text Sample {idx:02d}:-')\n",
    "    print(augmented_text[0])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  fluffy brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline # noqa\n",
    "\n",
    "# Initialize the fill-mask pipeline\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"distilroberta-base\",\n",
    "    tokenizer=\"distilroberta-base\"\n",
    ")\n",
    "\n",
    "def replace_word_with_masked_token(sentence, word_to_replace):\n",
    "    # Replace the word with the masked token\n",
    "    masked_sentence = sentence.replace(word_to_replace, fill_mask.tokenizer.mask_token)\n",
    "    \n",
    "    # Use the fill-mask pipeline to predict the masked token\n",
    "    predictions = fill_mask(masked_sentence)\n",
    "    \n",
    "    # Replace the masked token with the top prediction\n",
    "    augmented_sentence = masked_sentence.replace(fill_mask.tokenizer.mask_token, predictions[0][\"token_str\"])\n",
    "    \n",
    "    return augmented_sentence\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "word_to_replace = \"quick\"\n",
    "print(replace_word_with_masked_token(sentence, word_to_replace))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"quick brown fox jumps over the lazy dog\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog .\n",
      "\n",
      "Augmented Text Sample by replacing: \"quick\":-\n",
      "The  fluffy brown fox jumps over the lazy dog\n",
      "\n",
      "Augmented Text Sample by replacing: \"brown\":-\n",
      "The quick  eyed fox jumps over the lazy dog\n",
      "\n",
      "Augmented Text Sample by replacing: \"jumps\":-\n",
      "The quick brown fox  wins over the lazy dog\n",
      "\n",
      "Augmented Text Sample by replacing: \"lazy\":-\n",
      "The quick brown fox jumps over the  barking dog\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\")\n",
    "print(text)\n",
    "words_to_replace = ['quick', 'brown', 'jumps', 'lazy']\n",
    "\n",
    "for idx, word_to_replace in enumerate(words_to_replace):\n",
    "    print()\n",
    "    print(f'Augmented Text Sample by replacing: \"{word_to_replace}\":-')\n",
    "    print(replace_word_with_masked_token(sentence, word_to_replace))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
